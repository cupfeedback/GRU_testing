{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# tensorflow 2.13.0"
      ],
      "metadata": {
        "id": "F-5LVUzdpO14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.13.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "MIS0zmiqhYDu",
        "outputId": "8ff420b1-d5eb-413c-e48d-2e429f3030ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.11.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (18.1.1)\n",
            "Collecting numpy<=1.24.3,>=1.22 (from tensorflow==2.13.0)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.4.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow==2.13.0)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.34 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.24.3 which is incompatible.\n",
            "pydantic 2.9.1 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.23.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.13.0 which is incompatible.\n",
            "torch 2.4.0+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.13.1 numpy-1.24.3 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "06fd092e05c846c6bcefbd7c70b6b0fd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# libraries"
      ],
      "metadata": {
        "id": "vfqwjXN8pSZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import traceback\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "import random as  rnd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ],
      "metadata": {
        "id": "wrCxGPcKlzGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NrG097VBgr0R",
        "outputId": "50076ae3-d747-4e6f-b01d-d58f13ec6a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.13.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "mLHDoXJapUh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 원본 파일 경로 및 새로 저장할 파일 경로 지정\n",
        "dirname = 'data/'\n",
        "filename = 'test_euc_kr.txt'\n",
        "output_filename = 'test_utf_8.txt'\n",
        "\n",
        "# CP949로 파일 읽기\n",
        "with open(os.path.join(dirname, filename), 'r', encoding='cp949') as infile:\n",
        "    content = infile.read()\n",
        "\n",
        "# UTF-8로 파일 쓰기\n",
        "with open(os.path.join(dirname, output_filename), 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(content)\n",
        "\n",
        "print(f\"파일이 {filename}에서 {output_filename}(으)로 변환되었습니다.\")"
      ],
      "metadata": {
        "id": "7yYe-tdWqw7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어린 왕자와 위대한 유산로 test\n",
        "dirname = 'data/'\n",
        "filename = 'test_utf_8.txt'\n",
        "lines = [] # storing all the lines in a variable.\n",
        "\n",
        "counter = 0\n",
        "\n",
        "with open(os.path.join(dirname, filename), encoding='utf-8') as files:\n",
        "    for line in files:\n",
        "        # remove leading and trailing whitespace\n",
        "        pure_line = line.strip()#.lower()\n",
        "\n",
        "        # if pure_line is not the empty string,\n",
        "        if pure_line:\n",
        "            # append it to the list\n",
        "            lines.append(pure_line)\n",
        "\n",
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xevj0YsFfbfj",
        "outputId": "7e072414-6c20-450e-b4a3-ef1da932ab00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 9520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 create the vocabulary"
      ],
      "metadata": {
        "id": "iwThDwa2q6WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\\n\".join(lines)\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "vocab.insert(0,\"[UNK]\") # Add a special character for any unknown\n",
        "vocab.insert(1,\"\") # Add the empty character for padding.\n",
        "\n",
        "print(f'{len(vocab)} unique characters')\n",
        "print(\" \".join(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUXhOWvnfp3R",
        "outputId": "7c16a7e6-87b8-4a90-e04c-d446ade04195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1594 unique characters\n",
            "[UNK]  \t \n",
            "   ! \" # % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A B C D E F G H I J K L M N O P R S T U V W X Y Z [ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z ~ × ― ‘ ’ “ ” ‥ … ※ ℓ ← ↑ → ↓ ↔ ≒ ■ ▲ △ ▶ ▼ ◈ ○ ★ ♡ ♥ 〈 〉 「 」 『 』 〓 ㄱ ㅂ ㅅ ㅈ ㅋ ㅌ ㅍ ㅎ ㅜ ㅠ ㎏ ㎖ 使 儀 商 大 小 布 式 律 惑 星 的 紀 紅 詩 隊 가 각 간 갇 갈 갉 감 갑 값 갓 갔 강 갖 갗 같 갚 갛 개 객 갠 갤 갯 갰 걀 거 걱 건 걷 걸 걺 검 겁 것 겉 겊 겋 게 겐 겝 겠 겨 격 겪 견 결 겸 겹 겼 경 곁 계 곗 고 곡 곤 곧 골 곪 곯 곰 곱 곳 공 과 곽 관 괄 광 괘 괜 괴 괸 굉 교 구 국 군 굳 굴 굵 굶 굼 굽 굿 궁 궂 권 궐 궤 귀 귄 귈 귓 규 균 그 극 근 글 긁 금 급 긋 긍 기 긴 길 김 깁 깃 깊 까 깍 깎 깐 깔 깜 깝 깟 깡 깥 깨 깬 깰 깻 깼 깽 꺼 꺽 꺾 껄 껌 껍 껏 껐 껑 께 껜 껴 꼈 꼬 꼭 꼴 꼼 꼽 꼿 꽁 꽂 꽃 꽉 꽝 꽤 꽥 꾀 꾐 꾸 꾼 꿀 꿇 꿈 꿋 꿍 꿔 꿨 꿰 뀌 뀐 뀔 끄 끈 끊 끌 끓 끔 끗 끙 끝 끼 끽 낀 낄 낌 낍 낏 나 낙 낚 난 날 낡 남 납 낫 났 낭 낮 낯 낱 낳 내 낸 낼 냄 냅 냈 냉 냐 냔 냘 냥 너 넉 넋 넌 널 넓 넘 넛 넜 넝 넣 네 넥 넨 넬 넵 넷 넸 녀 녁 년 념 녔 녕 녘 녜 노 녹 논 놀 놈 놋 농 높 놓 놔 놨 뇌 뇨 누 눅 눈 눌 눔 눕 눠 눴 뉘 뉜 뉴 늄 늉 느 늑 는 늘 늙 늠 능 늦 늪 늬 니 닉 닌 닐 님 닙 닛 닝 닢 다 닥 닦 단 닫 달 닭 닮 닳 담 답 닷 당 닻 닿 대 댁 댄 댈 댐 댓 댔 댕 더 덕 던 덜 덟 덤 덥 덧 덩 덫 덮 데 덴 델 뎁 뎅 뎌 뎠 도 독 돈 돋 돌 돔 돕 동 돛 돼 됐 되 된 될 됨 됩 두 둑 둔 둘 둠 둡 둥 둬 뒀 뒈 뒤 뒷 뒹 드 득 든 듣 들 듦 듬 듭 듯 등 디 딕 딘 딛 딜 딤 딩 딪 따 딱 딴 딸 땀 땁 땅 땋 때 땐 땔 땜 땠 땡 떠 떡 떤 떨 떳 떴 떻 떼 뗀 뗄 뗏 또 똑 똘 똥 똬 뚜 뚝 뚤 뚫 뚱 뛰 뛴 뛸 뜨 뜩 뜬 뜯 뜰 뜸 뜻 띄 띈 띌 띔 띠 띤 띨 라 락 란 랄 람 랍 랐 랑 랗 래 랙 랜 램 랩 랫 랬 랭 랴 략 량 러 럭 런 럴 럼 럽 럿 렀 렁 렇 레 렉 렌 렛 려 력 련 렬 렴 렵 렷 렸 령 례 로 록 론 롤 롬 롭 롯 롱 뢰 료 룡 루 룩 룬 룰 룸 룻 뤄 류 륙 륜 률 륭 르 륵 른 를 름 릅 릇 릉 릎 리 릭 린 릴 림 립 릿 링 마 막 만 많 말 맑 맘 맙 맛 망 맞 맡 맣 매 맥 맨 맬 맴 맵 맷 맸 맹 맺 머 먹 먼 멀 멈 멋 멍 멎 멓 메 멘 멜 멤 멧 멩 며 멱 면 멸 몄 명 몇 모 목 몫 몬 몰 몸 몹 못 몽 묘 무 묵 묶 문 묻 물 뭄 뭇 뭉 뭐 뭔 뭘 뭡 뭣 뭬 뮤 므 믄 믈 미 믹 민 믿 밀 밈 밉 밋 밌 밍 및 밑 바 박 밖 반 받 발 밝 밟 밤 밥 밧 방 밭 배 백 밴 뱀 뱃 뱅 뱉 버 벅 번 벌 범 법 벗 벙 베 벤 벨 벳 벼 벽 변 별 볍 볐 병 볕 보 복 볶 본 볼 봄 봅 봉 봐 봤 봬 뵈 뵌 뵐 뵙 부 북 분 불 붉 붐 붓 붕 붙 뷰 브 블 비 빈 빌 빔 빗 빙 빚 빛 빠 빡 빤 빨 빳 빴 빵 빻 빼 빽 뺀 뺄 뺏 뺑 뺨 뻐 뻑 뻔 뻗 뻘 뻣 뻤 뼈 뼉 뼘 뼛 뽀 뽐 뽑 뾰 뿌 뿍 뿐 뿔 뿜 쀼 쁘 쁜 쁠 쁨 쁩 삐 삑 사 삭 산 살 삶 삼 삽 샀 상 샅 새 색 샌 샐 샘 샛 생 샤 샴 샵 샹 서 석 섞 선 설 섬 섭 섯 섰 성 세 섹 센 셀 셈 셉 셋 셌 셔 션 셨 셰 소 속 손 솔 솜 솟 송 솥 쇄 쇠 쇳 쇼 숄 숍 숏 수 숙 순 술 숨 숫 숭 숯 숱 숲 숴 쉬 쉰 쉴 쉼 쉽 쉿 슈 스 슥 슨 슬 슭 슴 습 슷 승 시 식 신 싣 실 싫 심 십 싯 싱 싲 싶 싸 싹 싼 쌀 쌌 쌍 쌓 쌔 쌜 쌤 쌩 써 썩 썰 썸 썹 썼 썽 쎄 쏘 쏙 쏜 쏟 쏠 쏴 쐐 쐬 쐴 쑤 쑥 쓰 쓱 쓴 쓸 씀 씃 씌 씨 씩 씬 씸 씹 씻 씽 아 악 안 앉 않 알 앓 암 압 앗 았 앙 앞 애 액 앤 앨 앳 앴 앵 야 약 얀 얄 얇 얌 양 얕 얗 얘 어 억 언 얹 얻 얼 얽 엄 업 없 엇 었 엉 엌 엎 에 엑 엔 엘 엡 엣 엥 여 역 엮 연 열 엷 염 엽 엾 엿 였 영 옅 옆 옇 예 옛 오 옥 온 올 옭 옮 옳 옴 옵 옷 옹 와 왁 완 왈 왔 왕 왜 왠 외 왼 요 욕 용 우 욱 운 울 움 웁 웃 웅 워 원 월 웝 웠 웨 웬 웰 웹 위 윈 윌 윔 윕 윗 윙 유 육 윤 율 융 으 윽 은 을 읊 음 읍 읏 응 읕 의 읨 이 익 인 일 읽 잃 임 입 잇 있 잉 잊 잎 자 작 잔 잖 잘 잠 잡 잤 장 잦 재 잭 잰 잼 잽 잿 쟁 저 적 전 절 젊 점 접 젓 정 젖 제 젝 젠 젯 져 졌 조 족 존 졸 좀 좁 종 좋 좌 죄 죠 죵 주 죽 준 줄 줌 줍 줏 중 줘 줬 쥐 쥔 쥘 즈 즉 즌 즐 즘 즙 증 지 직 진 질 짐 집 짓 징 짖 짙 짚 짜 짝 짠 짢 짤 짧 짬 짰 짱 째 짹 짼 쨌 쨍 쩌 쩍 쩐 쩔 쩜 쩝 쩡 쪘 쪼 쪽 쫌 쫒 쫓 쬐 쭈 쭉 쭌 쭐 쭙 쭤 쯤 쯧 찌 찍 찐 찔 찜 찝 찢 찧 차 착 찬 찮 찰 참 찻 찼 창 찾 채 책 챈 챌 챘 챙 처 척 천 철 첨 첩 첫 청 체 첼 쳐 쳤 초 촉 촌 촐 촘 촛 총 최 쵸 추 축 춘 출 춤 춥 춧 충 춰 췄 췌 취 췬 츠 측 츰 층 치 칙 친 칠 침 칫 칭 카 칵 칸 칼 캄 캐 캔 캠 캥 캭 커 컥 컨 컬 컴 컵 컷 컸 컹 케 켄 켓 켜 켠 켤 켰 코 콘 콜 콤 콧 콩 콸 쾅 쾌 쿠 쿡 쿨 쿵 퀴 큐 크 큰 클 큼 키 킥 킨 킬 킴 킷 킹 타 탁 탄 탈 탐 탑 탓 탔 탕 태 택 탠 탬 탰 탱 터 턱 턴 털 텀 텁 텃 텄 텅 테 텐 텔 템 텨 토 톡 톤 톨 톰 톱 통 퇘 퇴 투 툭 툰 툴 툼 퉁 튀 튄 튈 튜 트 특 튼 튿 틀 틈 틋 틔 티 틱 틴 틸 팀 팁 팅 파 팍 팎 판 팔 팠 팡 패 팩 팬 팸 팻 팽 퍼 퍽 펀 펄 펌 펍 펐 펑 페 펜 펠 펫 펴 편 펼 폈 평 폐 포 폭 폰 폴 폼 폿 표 푸 푹 푼 풀 품 풋 풍 프 픈 플 픔 피 핀 필 핌 핍 핏 핑 하 학 한 할 핥 함 합 핫 항 해 핵 핸 핼 햄 햇 했 행 향 허 헉 헌 헐 험 헛 헝 헤 헨 헴 헷 헸 헹 혀 혁 현 혈 혐 협 혓 혔 형 혜 호 혹 혼 홀 홉 홍 화 확 환 활 황 홱 횃 횅 회 획 횟 횡 효 후 훈 훌 훑 훔 훗 훤 훨 훼 휑 휘 휙 휜 휩 휴 흉 흐 흑 흔 흘 흙 흠 흡 흥 흩 희 흰 히 힌 힐 힘 힙\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Convert a Line to Tensor"
      ],
      "metadata": {
        "id": "EfZl838WrAki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line = \"안녕하세요!\"\n",
        "chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdY0bxDVrXbv",
        "outputId": "6e18884d-1c80-4bfb-8a05-ed48af3eeb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xec\\x95\\x88' b'\\xeb\\x85\\x95' b'\\xed\\x95\\x98' b'\\xec\\x84\\xb8'\n",
            " b'\\xec\\x9a\\x94' b'!'], shape=(6,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자로 출력\n",
        "decoded_chars = [char.numpy().decode('utf-8') for char in chars]\n",
        "\n",
        "print(decoded_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rsseQN-rX4o",
        "outputId": "bc9d4a32-864d-4110-93fb-52e73d077d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안', '녕', '하', '세', '요', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1S5JbR4rb1F",
        "outputId": "3ddad08d-a3f6-4c35-9423-4dc4aee12ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1005  386 1502  893 1093    5], shape=(6,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: line_to_tensor\n",
        "def line_to_tensor(line, vocab):\n",
        "    \"\"\"\n",
        "    Converts a line of text into a tensor of integer values representing characters.\n",
        "\n",
        "    Args:\n",
        "        line (str): A single line of text.\n",
        "        vocab (list): A list containing the vocabulary of unique characters.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor(dtype=int64): A tensor containing integers (unicode values) corresponding to the characters in the `line`.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Split the input line into individual characters\n",
        "    chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
        "    # Map characters to their respective integer values using StringLookup\n",
        "    ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return ids"
      ],
      "metadata": {
        "id": "fgrVRz8UrFH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_ids = line_to_tensor('반갑 습니다', vocab)\n",
        "print(f\"Result: {tmp_ids}\")\n",
        "print(f\"Output type: {type(tmp_ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0DhXUAzrFYx",
        "outputId": "d14c5b2b-7d61-42e1-b791-ebe4b827ec12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: [747 160   4 944 425 434]\n",
            "Output type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids, vocab):\n",
        "    \"\"\"\n",
        "    Converts a tensor of integer values into human-readable text.\n",
        "\n",
        "    Args:\n",
        "        ids (tf.Tensor): A tensor containing integer values (unicode IDs).\n",
        "        vocab (list): A list containing the vocabulary of unique characters.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the characters in human-readable format.\n",
        "    \"\"\"\n",
        "    # Initialize the StringLookup layer to map integer IDs back to characters\n",
        "    chars_from_ids = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n",
        "\n",
        "    # Use the layer to decode the tensor of IDs into human-readable text\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "t70AmqZgrFhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_from_ids(ids, vocab).numpy().decode('utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SlXOPeXBrOE7",
        "outputId": "cb6061d0-44e8-4674-d5b8-537948abe9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ure4v7y2rOQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8UwbfHzorOTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Prepare your data for training and testing"
      ],
      "metadata": {
        "id": "5olt_2WPrhZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = lines[:-1000] # Leave the rest for training\n",
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "\n",
        "print(f\"Number of training lines: {len(train_lines)}\")\n",
        "print(f\"Number of validation lines: {len(eval_lines)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oChUu8JAfqkm",
        "outputId": "76f6e506-e38d-49bd-d817-402b8f69acd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training lines: 8520\n",
            "Number of validation lines: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 TensorFlow dataset"
      ],
      "metadata": {
        "id": "CpqW4fqKrnOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = line_to_tensor(\"\\n\".join([\"안녕하세요!\", \"생성형 AI\"]), vocab)\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8spqvBMlrqRq",
        "outputId": "69797bc1-10c6-4b4f-eed6-a8cfecd5497b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(13,), dtype=int64, numpy=\n",
              "array([1005,  386, 1502,  893, 1093,    5,    3,  878,  892, 1541,    4,\n",
              "         36,   44])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print([text_from_ids([ids], vocab).numpy().decode('utf-8') for ids in ids_dataset.take(10)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZM_CoyurqY6",
        "outputId": "a573577c-18b6-4df0-faaa-ae35b86d323a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안', '녕', '하', '세', '요', '!', '\\n', '생', '성', '형']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 10\n",
        "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "oL9bzZHvrxu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in data_generator.take(2):\n",
        "    print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj4b4QJ4r0Rd",
        "outputId": "833011a3-0a58-4adc-df6f-6ae190a99761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1005  386 1502  893 1093    5    3  878  892 1541    4], shape=(11,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for seq in data_generator.take(2):\n",
        "    print(f\"{i}. {text_from_ids(seq, vocab).numpy().decode('utf-8')}\")\n",
        "    i = i + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjyQ_MIzr0VL",
        "outputId": "cf46967c-5bb6-4741-edfc-00054ac6e1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 안녕하세요!\n",
            "생성형 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Create the input and the output for your model"
      ],
      "metadata": {
        "id": "YVUo7G5tr7qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    \"\"\"\n",
        "    Splits the input sequence into two sequences, where one is shifted by one position.\n",
        "\n",
        "    Args:\n",
        "        sequence (tf.Tensor or list): A list of characters or a tensor.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor, tf.Tensor: Two tensors representing the input and output sequences for the model.\n",
        "    \"\"\"\n",
        "    # Create the input sequence by excluding the last character\n",
        "    input_text = sequence[:-1]\n",
        "    # Create the target sequence by excluding the first character\n",
        "    target_text = sequence[1:]\n",
        "\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "WmXslKcWr0Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"텐서플로우\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkaBRhvVr_tu",
        "outputId": "566eec14-da79-4986-d8d6-f57c759beac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['텐', '서', '플', '로'], ['서', '플', '로', '우'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: create_batch_dataset\n",
        "def create_batch_dataset(lines, vocab, seq_length=100, batch_size=64):\n",
        "    \"\"\"\n",
        "    Creates a batch dataset from a list of text lines.\n",
        "\n",
        "    Args:\n",
        "        lines (list): A list of strings with the input data, one line per row.\n",
        "        vocab (list): A list containing the vocabulary.\n",
        "        seq_length (int): The desired length of each sample.\n",
        "        batch_size (int): The batch size.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A batch dataset generator.\n",
        "    \"\"\"\n",
        "    # Buffer size to shuffle the dataset\n",
        "    # (TF data is designed to work with possibly infinite sequences,\n",
        "    # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "    # it maintains a buffer in which it shuffles elements).\n",
        "    BUFFER_SIZE = 10000\n",
        "\n",
        "    # For simplicity, just join all lines into a single line\n",
        "    single_line_data  = \"\\n\".join(lines)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Convert your data into a tensor using the given vocab\n",
        "    all_ids = line_to_tensor(single_line_data, vocab)\n",
        "    # Create a TensorFlow dataset from the data tensor\n",
        "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "    # Create a batch dataset\n",
        "    data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "    # Map each input sample using the split_input_target function\n",
        "    dataset_xy = data_generator.map(split_input_target)\n",
        "\n",
        "    # Assemble the final dataset with shuffling, batching, and prefetching\n",
        "    dataset = (\n",
        "        dataset_xy\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(batch_size, drop_remainder=True)\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Cpw_dIgzsBx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your function\n",
        "tf.random.set_seed(1)\n",
        "dataset = create_batch_dataset(train_lines[1:100], vocab, seq_length=16, batch_size=2)\n",
        "\n",
        "print(\"Prints the elements into a single batch. The batch contains 2 elements: \")\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"\\n\\033[94mInput0\\t:\", text_from_ids(input_example[0], vocab).numpy().decode('utf-8'))\n",
        "    print(\"\\n\\033[90mTarget0\\t:\", text_from_ids(target_example[0], vocab).numpy().decode('utf-8'))\n",
        "\n",
        "    print(\"\\n\\n\\033[94mInput1\\t:\", text_from_ids(input_example[1], vocab).numpy().decode('utf-8'))\n",
        "    print(\"\\n\\033[90mTarget1\\t:\", text_from_ids(target_example[1], vocab).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl8h2KEpsB14",
        "outputId": "0b8f1658-c2ea-4a85-845b-4c3c1b0df912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prints the elements into a single batch. The batch contains 2 elements: \n",
            "\n",
            "\u001b[94mInput0\t: 포켓에서 내가 그려 준 양의 \n",
            "\n",
            "\u001b[90mTarget0\t: 켓에서 내가 그려 준 양의 그\n",
            "\n",
            "\n",
            "\u001b[94mInput1\t: 아이 같은 구석이라고는 없었다\n",
            "\n",
            "\u001b[90mTarget1\t: 이 같은 구석이라고는 없었다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Create the training dataset"
      ],
      "metadata": {
        "id": "AD3aqvr2sRri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "dataset = create_batch_dataset(train_lines, vocab, seq_length=100, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "B4z_PW_RsB5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Defining the GRU Language Model (GRULM)"
      ],
      "metadata": {
        "id": "IJMJfP6TsYFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED CLASS: GRULM\n",
        "class GRULM(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    A GRU-based language model that maps from a tensor of tokens to activations over a vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        embedding_dim (int, optional): Depth of embedding. Defaults to 256.\n",
        "        rnn_units (int, optional): Number of units in the GRU cell. Defaults to 128.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: A GRULM language model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units=128):\n",
        "        super().__init__(self)\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # Create an embedding layer to map token indices to embedding vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define a GRU (Gated Recurrent Unit) layer for sequence modeling\n",
        "        self.gru = tf.keras.layers.GRU(units=rnn_units, return_sequences=True, return_state=True)\n",
        "        # Apply a dense layer with log-softmax activation to predict next tokens\n",
        "        self.dense = tf.keras.layers.Dense(units=vocab_size, activation=tf.nn.log_softmax)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        # Map input tokens to embedding vectors\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            # Get initial state from the GRU layer\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        # Predict the next tokens and apply log-softmax activation\n",
        "        x = self.dense(x, training=training)\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "IP2fAE93f-WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = 1594\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 512\n",
        "\n",
        "# RNN layers\n",
        "rnn_units = 1024\n",
        "\n",
        "model = GRULM(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units = rnn_units)"
      ],
      "metadata": {
        "id": "_YuD00SEsfRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing your model\n",
        "\n",
        "try:\n",
        "    # Simulate inputs of length 100. This allows to compute the shape of all inputs and outputs of our network\n",
        "    model.build(input_shape=(BATCH_SIZE, 100))\n",
        "    model.call(Input(shape=(100)))\n",
        "    model.summary()\n",
        "except:\n",
        "    print(\"\\033[91mError! \\033[0mA problem occurred while building your model. This error can occur due to wrong initialization of the return_sequences parameter\\n\\n\")\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q3TNIoUshRQ",
        "outputId": "fdce378c-8f1b-45cd-f61e-c6ab028e13d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"grulm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 512)          816128    \n",
            "                                                                 \n",
            " gru (GRU)                   [(None, 100, 1024),       4724736   \n",
            "                              (None, 1024)]                      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100, 1594)         1633850   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7174714 (27.37 MB)\n",
            "Trainable params: 7174714 (27.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
        "    example_batch_predictions = model(tf.constant([input_example_batch[0].numpy()]))\n",
        "    print(\"\\n\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIgNsiAgslS0",
        "outputId": "120b99e6-5a7f-44c7-df2d-9be16a83442e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  [1449  197 1128    4  748 1013  437  434   17    4 1537 1135  644    4\n",
            "  748 1127    4  185  660    4 1511  475    4  352  153    4 1360 1495\n",
            "  940   11  862  257  309   12  216 1135    4  218  203  644    4  203\n",
            "  906 1517  459    4   23   92   24  383    4 1005 1048    4   21  766\n",
            " 1139  153    4   22  766    4 1175  475 1065  437  434   17    4 1502\n",
            " 1215  660    4  703  491    4 1214  177  799 1184 1125  618    4    4\n",
            " 1487  608  346 1044 1215   17    3  197  228   15    4  337  417    4\n",
            " 1360 1495]\n",
            "\n",
            " (1, 100, 1594) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output size is (1, 100, 82). We predicted only on the first sequence generated by the batch generator. 100 is the number of predicted characters. It has exactly the same length as the input. And there are 82 values for each predicted character. Each of these 82 real values are related to the logarithm likelihood of each character to be the next one in the sequence. The bigger the value, the higher the likelihood. As the network is not trained yet, all those values must be very similar and random. Just check the values for the last prediction on the sequence."
      ],
      "metadata": {
        "id": "_wHJaj9jsrpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions[0][99].numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RHlJ2ZUsuA3",
        "outputId": "16319bf2-618c-41f7-f808-0b91f9d7d660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-7.3532653, -7.376057 , -7.3669114, ..., -7.3824162, -7.3772197,\n",
              "       -7.379995 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_character = tf.math.argmax(example_batch_predictions[0][99])\n",
        "print(last_character.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O-T0fXNsujo",
        "outputId": "2f66d3af-fd5b-49e9-fca1-9b35e772a1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.math.argmax(example_batch_predictions[0], axis=1)\n",
        "print(sampled_indices.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1qRFC3ksuoA",
        "outputId": "dd2e0320-5c37-4bd6-953d-d57f78c31835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 649  766  232 1587  895 1143 1113 1362 1378  788 1079 1389 1366  614\n",
            " 1267  517 1216  702  233  181 1337  747  360 1445 1445  360   32  204\n",
            " 1002 1018 1426 1508 1130 1010  768 1366 1068  970   43 1225  614   43\n",
            "  981  858 1478  832 1018  791  954  149  788 1570  742  263  658 1139\n",
            "  381 1533 1068  677 1381  263 1496  747  369 1327 1040  805  788 1519\n",
            "  845  233  263  970 1527 1587  588  117  401  534  409 1474  263  263\n",
            "   29  664  562 1151  797  610 1434  588 1488  983  364  718 1231 1579\n",
            "  134  204]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0], vocab).numpy().decode('utf-8'))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices, vocab).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDIJAnT1surf",
        "outputId": "91166252-dc1a-44ff-f698-48259a8bc8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " 판결을 받았단다. 혐의를 받은 것만 해도 내가 콤피슨(사기꾼)과의 관계를 계속했던 4~5년 안에 2번인가 3번 정도였단다. 하지만 모두 증거부족으로  풀려났었지.\n",
            "결국, 나는 콤피\n",
            "\n",
            "Next Char Predictions:\n",
            " 릎번굵희센임위콩킨볼옴탓쿠렷찝딛직몇굶걸캔반냔팅팅냔=곗씽앤퉁합음암범쿠옆써H짚렷H쏟쁨폭뺑앤봉십紀볼훼및깊마인녀혀옆맷킷깊핀반넘칠엄붓볼향뽀굶깊써헤희랫♡뇨땠눴폈깊깊:맘뜬작뵐련튿랫품쏴넉물짧흑ㅜ곗\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Training"
      ],
      "metadata": {
        "id": "EF0hTf6ns1gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: Compile model\n",
        "\n",
        "def compile_model(model):\n",
        "    \"\"\"\n",
        "    Sets the loss and optimizer for the given model\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): The model to compile.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The compiled model.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Define the loss function. Use SparseCategoricalCrossentropy\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # Define and Adam optimizer\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.00125)\n",
        "    # Compile the model using the parametrized Adam optimizer and the SparseCategoricalCrossentropy funcion\n",
        "    model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "2zERakEksuul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "# Compile the model\n",
        "model = compile_model(model)\n",
        "# Fit the model\n",
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4a-4MECsuxZ",
        "outputId": "00a587cb-cbb9-451b-f1ab-e8685b538a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "126/126 [==============================] - 108s 839ms/step - loss: 4.4204\n",
            "Epoch 2/30\n",
            "126/126 [==============================] - 105s 829ms/step - loss: 3.2294\n",
            "Epoch 3/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 2.8860\n",
            "Epoch 4/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 2.6825\n",
            "Epoch 5/30\n",
            "126/126 [==============================] - 105s 829ms/step - loss: 2.5287\n",
            "Epoch 6/30\n",
            "126/126 [==============================] - 106s 830ms/step - loss: 2.4015\n",
            "Epoch 7/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 2.2872\n",
            "Epoch 8/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 2.1752\n",
            "Epoch 9/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 2.0655\n",
            "Epoch 10/30\n",
            "126/126 [==============================] - 104s 821ms/step - loss: 1.9527\n",
            "Epoch 11/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 1.8383\n",
            "Epoch 12/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 1.7191\n",
            "Epoch 13/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 1.5975\n",
            "Epoch 14/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 1.4754\n",
            "Epoch 15/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 1.3563\n",
            "Epoch 16/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 1.2430\n",
            "Epoch 17/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 1.1405\n",
            "Epoch 18/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 1.0471\n",
            "Epoch 19/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 0.9672\n",
            "Epoch 20/30\n",
            "126/126 [==============================] - 105s 824ms/step - loss: 0.8984\n",
            "Epoch 21/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 0.8404\n",
            "Epoch 22/30\n",
            "126/126 [==============================] - 105s 824ms/step - loss: 0.7914\n",
            "Epoch 23/30\n",
            "126/126 [==============================] - 105s 826ms/step - loss: 0.7482\n",
            "Epoch 24/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 0.7132\n",
            "Epoch 25/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 0.6850\n",
            "Epoch 26/30\n",
            "126/126 [==============================] - 105s 823ms/step - loss: 0.6577\n",
            "Epoch 27/30\n",
            "126/126 [==============================] - 105s 824ms/step - loss: 0.6368\n",
            "Epoch 28/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 0.6189\n",
            "Epoch 29/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 0.5983\n",
            "Epoch 30/30\n",
            "126/126 [==============================] - 105s 822ms/step - loss: 0.5839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 경로 설정\n",
        "output_dir = './model_test_512/model.h5'\n",
        "\n",
        "# 모델 전체 저장 (구조 + 가중치 + 컴파일 정보)\n",
        "model.save(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "R3u9bDS1tAU8",
        "outputId": "c55d1a9f-03b4-4320-b1de-ae6c02018989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3ed8c76b99a0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 모델 전체 저장 (구조 + 가중치 + 컴파일 정보)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         ):\n\u001b[0;32m--> 152\u001b[0;31m             raise NotImplementedError(\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;34m\"Saving the model to HDF5 format requires the model to be a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;34m\"Functional model or a Sequential model. It does not work for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SavedModel 형식으로 모델 전체 저장\n",
        "output_dir = './model_test_512/saved_model_2'\n",
        "model.save(output_dir, save_format='tf')"
      ],
      "metadata": {
        "id": "6W65ZNxiRogM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치만 저장\n",
        "# output_dir = './model_test_512/checkpoint_weights.h5'\n",
        "# model.save_weights(output_dir)"
      ],
      "metadata": {
        "id": "wUcVPxAQtBd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 모델을 불러올 경우\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 모델 불러오기 (HDF5 형식 또는 SavedModel 형식)\n",
        "model = load_model('./model_test_512/model.h5')  # 또는 saved_model 디렉토리 경로"
      ],
      "metadata": {
        "id": "ByvgzXY0ui0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치만 불러올 경우\n",
        "# 동일한 모델 구조 정의\n",
        "model = compile_model(your_model_definition)\n",
        "\n",
        "# 가중치 로드\n",
        "model.load_weights('./model_test_512/checkpoint_weights.h5')"
      ],
      "metadata": {
        "id": "Z6cwAoSRuxJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Evaluation\n",
        "- 4.1 Evaluating using the Deep Nets\n",
        "- log perplexity"
      ],
      "metadata": {
        "id": "TFjXgr_7tUJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: log_perplexity\n",
        "def log_perplexity(preds, target):\n",
        "    \"\"\"\n",
        "    Function to calculate the log perplexity of a model.\n",
        "\n",
        "    Args:\n",
        "        preds (tf.Tensor): Predictions of a list of batches of tensors corresponding to lines of text.\n",
        "        target (tf.Tensor): Actual list of batches of tensors corresponding to lines of text.\n",
        "\n",
        "    Returns:\n",
        "        float: The log perplexity of the model.\n",
        "    \"\"\"\n",
        "    PADDING_ID = 1\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Calculate log probabilities for predictions using one-hot encoding\n",
        "    log_p = np.sum(tf.one_hot(target, depth=preds.shape[-1]) * preds, axis= -1) # HINT: tf.one_hot(...) should replace one of the Nones\n",
        "    # Identify non-padding elements in the target\n",
        "    non_pad = 1.0 - np.equal(target, PADDING_ID)          # You should check if the target equals to PADDING_ID\n",
        "    # Apply non-padding mask to log probabilities to exclude padding\n",
        "    log_p = log_p * non_pad                             # Get rid of the padding\n",
        "    # Calculate the log perplexity by taking the sum of log probabilities and dividing by the sum of non-padding elements\n",
        "    log_ppx = np.sum(log_p, axis=-1) / np.sum(non_pad, axis=-1) # Remember to set the axis properly when summing up\n",
        "    # Compute the mean of log perplexity\n",
        "    log_ppx = np.mean(log_ppx) # Compute the mean of the previous expression\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return -log_ppx"
      ],
      "metadata": {
        "id": "ljkGA0iziYlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model. This step is optional.\n",
        "# 모델 불러오는 코드\n",
        "# vocab_size = len(vocab)\n",
        "# embedding_dim = 512\n",
        "# rnn_units = 1024\n",
        "\n",
        "# model = GRULM(\n",
        "#     vocab_size=vocab_size,\n",
        "#     embedding_dim=embedding_dim,\n",
        "#     rnn_units = rnn_units)\n",
        "# model.build(input_shape=(100, vocab_size))\n",
        "# model.load_weights('./model_test_512/checkpoint_weights.h5')"
      ],
      "metadata": {
        "id": "M8v888g4fx44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for line in eval_lines[1:3]:\n",
        "eval_text = \"\\n\".join(eval_lines)\n",
        "eval_ids = line_to_tensor([eval_text], vocab)\n",
        "input_ids, target_ids = split_input_target(tf.squeeze(eval_ids, axis=0))\n",
        "\n",
        "preds, status = model(tf.expand_dims(input_ids, 0), training=False, states=None, return_state=True)\n",
        "\n",
        "#Get the log perplexity\n",
        "log_ppx = log_perplexity(preds, tf.expand_dims(target_ids, 0))\n",
        "print(f'The log perplexity and perplexity of your model are {log_ppx} and {np.exp(log_ppx)} respectively')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9EKAY_ft6xk",
        "outputId": "7102f0f6-57a1-4038-d2af-a1ca32184b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The log perplexity and perplexity of your model are 4.027871971462314 and 56.14131372104519 respectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Generating Language with your Own Model"
      ],
      "metadata": {
        "id": "jTU0Hli3t-rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def temperature_random_sampling(log_probs, temperature=1.0):\n",
        "    \"\"\"Temperature Random sampling from a categorical distribution. The higher the temperature, the more\n",
        "       random the output. If temperature is close to 0, it means that the model will just return the index\n",
        "       of the character with the highest input log_score\n",
        "\n",
        "    Args:\n",
        "        log_probs (tf.Tensor): The log scores for each characeter in the dictionary\n",
        "        temperature (number): A value to weight the random noise.\n",
        "    Returns:\n",
        "        int: The index of the selected character\n",
        "    \"\"\"\n",
        "   # Generate uniform random numbers with a slight offset to avoid log(0)\n",
        "    u = tf.random.uniform(minval=1e-6, maxval=1.0 - 1e-6, shape=log_probs.shape)\n",
        "\n",
        "    # Apply the Gumbel distribution transformation for randomness\n",
        "    g = -tf.math.log(-tf.math.log(u))\n",
        "\n",
        "    # Adjust the logits with the temperature and choose the character with the highest score\n",
        "    return tf.math.argmax(log_probs + g * temperature, axis=-1)"
      ],
      "metadata": {
        "id": "j6PmVrUQoSTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNGRADED CLASS: GenerativeModel\n",
        "class GenerativeModel(tf.keras.Model):\n",
        "    def __init__(self, model, vocab, temperature=1.0):\n",
        "        \"\"\"\n",
        "        A generative model for text generation.\n",
        "\n",
        "        Args:\n",
        "            model (tf.keras.Model): The underlying model for text generation.\n",
        "            vocab (list): A list containing the vocabulary of unique characters.\n",
        "            temperature (float, optional): A value to control the randomness of text generation. Defaults to 1.0.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        \"\"\"\n",
        "        Generate a single character and update the model state.\n",
        "\n",
        "        Args:\n",
        "            inputs (string): The input string to start with.\n",
        "            states (tf.Tensor): The state tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, states: The predicted character and the current GRU state.\n",
        "        \"\"\"\n",
        "        # Convert strings to token IDs.\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # Transform the inputs into tensors\n",
        "        input_ids = line_to_tensor(inputs, vocab)\n",
        "        # Predict the sequence for the given input_ids. Use the states and return_state=True\n",
        "        predicted_logits, states = self.model(input_ids, states, return_state=True)\n",
        "        # Get only last element of the sequence\n",
        "        predicted_logits = predicted_logits[0, -1, :]\n",
        "        # Use the temperature_random_sampling to generate the next character.\n",
        "        predicted_ids = temperature_random_sampling(predicted_logits, self.temperature)\n",
        "        # Use the chars_from_ids to transform the code into the corresponding char\n",
        "        predicted_chars = text_from_ids([predicted_ids], vocab)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return tf.expand_dims(predicted_chars, 0), states\n",
        "\n",
        "    def generate_n_chars(self, num_chars, prefix):\n",
        "        \"\"\"\n",
        "        Generate a text sequence of a specified length, starting with a given prefix.\n",
        "\n",
        "        Args:\n",
        "            num_chars (int): The length of the output sequence.\n",
        "            prefix (string): The prefix of the sequence (also referred to as the seed).\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text sequence.\n",
        "        \"\"\"\n",
        "        states = None\n",
        "        next_char = tf.constant([prefix])\n",
        "        result = [next_char]\n",
        "        for n in range(num_chars):\n",
        "            next_char, states = self.generate_one_step(next_char, states=states)\n",
        "            result.append(next_char)\n",
        "\n",
        "        return tf.strings.join(result)[0].numpy().decode('utf-8')"
      ],
      "metadata": {
        "id": "2Z97KiNhoWhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 testing"
      ],
      "metadata": {
        "id": "1VzVV4oDvLE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = GenerativeModel(model, vocab, temperature=0.0)\n",
        "\n",
        "print(gen.generate_n_chars(100, \" \"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(100, \"우물\"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(100, \"왕자\"), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGyHQyZhvHwX",
        "outputId": "c2918396-7ca7-4854-8348-43aa2fe6a1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                      \n",
            "\n",
            "________________________________________________________________________________\n",
            "우물 옆 으로 에스텔라의 아름다움에 빠지게 생긴 두개의 어느 아름답이다.\n",
            "그 하인이 마치 그녀의 옷깃에 달린 있었다. 그녀의 목소리는 모든 질문의 끝부분 속에 담겨두고 있었다. 그녀의 \n",
            "\n",
            "________________________________________________________________________________\n",
            "왕자가 물었다.\n",
            "\"지구는 별이니?\"\n",
            "\"없었기 때문이지. 그건 네 놈이 한 어디로 갈 거야.\"\n",
            "좀 전혀 없이 나는 대로 생각하게 웃었다.\n",
            "\"내 친구 여관이 바뀌었군!\"\n",
            "여우가 말했다.\n",
            " \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = GenerativeModel(model, vocab, temperature=0.0)\n",
        "\n",
        "print(gen.generate_n_chars(32, \" \"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"우물\"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"왕자\"), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR1LXaA0oYqC",
        "outputId": "e11d4551-e12f-4d5f-ddda-1fe0accb077a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 그녀가 내게  향했다.\n",
            "“예, 어르신.” 내가 말했다. “ \n",
            "\n",
            "________________________________________________________________________________\n",
            "우물의 등을 벽지리 (깨어야 할  말이 없는  중) 건물 내부 \n",
            "\n",
            "________________________________________________________________________________\n",
            "왕자, 에스텔라의 아이가 되었습니다.”\n",
            "“그래 나라.”라며 재 \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = GenerativeModel(model, vocab, temperature=.5)\n",
        "\n",
        "print(gen.generate_n_chars(32, \" \"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"우물\"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"왕자\"), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK0gXlnZo4Uz",
        "outputId": "c160e8ed-26b0-4bb8-afc4-4d06b3727a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 그녀가 그런 식으로 받아들이지 말고 당황해 그녀의 몸 어떤 \n",
            "\n",
            "________________________________________________________________________________\n",
            "우물의 등을 벽에 다시 한 번 서로 옆에서 꼭 기는 것이 그  \n",
            "\n",
            "________________________________________________________________________________\n",
            "왕자를 만나고 부어오른 듯 재거스 씨의 의자 반지에 도착할 수 \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = GenerativeModel(model, vocab, temperature=1.0)\n",
        "\n",
        "print(gen.generate_n_chars(32, \" \"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"우물\"), '\\n\\n' + '_'*80)\n",
        "print(gen.generate_n_chars(32, \"왕자\"), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s84GDPeko7sx",
        "outputId": "3032dcaa-98d4-4faf-cb3a-f2f0f28aa89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 펜을 담당하고 있었던 것 같았다. 내 생각에도 그녀의 말에 \n",
            "\n",
            "________________________________________________________________________________\n",
            "우물의 너머 한 곳에선 언어에 물을 쭉 돌며 말을 이어갔다.  \n",
            "\n",
            "________________________________________________________________________________\n",
            "왕자의 고개를 제기하고  있었다.\n",
            "내가 그녀에게 아무 일이 매 \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(np.random.randint(1, 1000))\n",
        "gen = GenerativeModel(model, vocab, temperature=0.8)\n",
        "import time\n",
        "start = time.time()\n",
        "print(gen.generate_n_chars(1000, \"나는 할 수 있어\"), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UsGOaNMoaXA",
        "outputId": "2953477c-64f0-4805-e373-ae04b3fc15d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나는 할 수 있어.” 조가 잠시 생각들을 수 있었다. “도둑들을 보는 순간 어디까지나 버렸죠?”\n",
            "“예 지금 내 말들 그가 맞아, 이 친구야. 내가 이 장치 무도에 오지 않던 테니까 아주 가까이 앞에 처치를 불안하게 만든 그러려는 아니겠어, 아니 아니야.”\n",
            "아니 이게 뭐라고 그런 건 아니지 못했지, 커피숍으로 들어가자, 난로를 잊어야 했다. ‘이 모든 것들을 구부지 않았을까?’라고 말한 후 우리를 여기 여기로 말했다. “불 소년 본 정장에 그 사람에게 이 소년을 즉시 부끄러울 게 하다 마세요. 그러니까 약간 당장 이걸 가지고 오른 손들을 가지고 돌아왔지요. 결혼 하나만 더 말이 더 있지 않을까요? 아니다면 이건 그의 머리가 어머니다.”\n",
            "“그렇게 해다오.” 내가 말했다. “일단 그 무시무시한 사람들 좀 더 주세요. 주저분의 사람들 말이다, 알현?  잘 살아.”\n",
            "그의 말에 따르면 “불가능한 그 일을 성실히 있는 것도, 9시나 바로 우리 집이 꼭 이야기를 제공하겠습니다. 잠시 여기저기가 멈추었는데 침이 “대장 안에 들었습니다.”\n",
            "“그렇지 않아,   그  애     우는  이  약간  자기 자네를 데려라   혼자.  그녀가  차가  언제  약점에   닿자,  내 대답을 기억해두시오.”\n",
            "“내 말이 맞아, 에스텔라!” 내가  말했다. “이쯤  되자 유행 관계에 있어서 일어나 좀 없는걸요. 무슨 의미심장이라는 것을 종종 언제든 내게 결혼식 갈 말만이지 않을 것일 뿐이지 않니?” 그가 도랑을 청했다.\n",
            "“이제 그 이야기는 것일세.” 재거스 씨가 말했다. “자 다른 질문은 너무도  한다.”\n",
            "그의 말에 대답  이상 가방이 반복 심하게 해드릴 수 없는  눈으로  나를  견고하게  되었다.  그가  대답했다.\n",
            "“제가 그 말씀이 있지 않다면, 사실 여기서 잠깐 기대어주십시오,  내  말을  믿고  싶다.”\n",
            "“그래, 그래, 연애하기 전부(차가운 내 눈)를  쳐다본 후  두  사람. 오늘 일이      다 그런 말  없으리라. 그건 저도 너를 본 적이 없어. 잠시 여기 여기서 듣기 전부터  어른들이 모두  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.710867404937744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(np.random.randint(1, 1000))\n",
        "gen = GenerativeModel(model, vocab, temperature=0.8)\n",
        "import time\n",
        "start = time.time()\n",
        "print(gen.generate_n_chars(1000, \"나는 할 수 있어\"), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARQrP-1vT5rD",
        "outputId": "a30896ad-23c3-4091-a253-93291a1c6b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나는 할 수 있어. 하지만 나와 상관없는 일도 유감스러웠을 것이며 어쨌든 그런 내가 어릴 적부터 어디에 사랑하고 있는지 하는 문제이 어디에 있지?”\n",
            "“어느 누구의 흔적은 생각할지에 대한 자본을 작고 오래갔고 당신은 차려하지 않았다. 내가 시간을 더 많이 지내던 그가 내 방으로 들어가자, 나를 칭적으로 받았다.\n",
            "내 이 상황은 앞서 말했듯이 그(웝슬 씨)가 계속 이렇게 지낼 수 있었다면 내가 열기 일보러지 않았기 때문에, 나는 전혀 내 전부(대치택)에 대하본 일이며 그보다 더 많은 얘(매그위)이나 한 성 씨 마냥 내게 글을 줄 수 있는 일을 하녀로 매번 일에 종종 일순간의 목소리 같은 얘기를 머릿속으로 밀어주더니 심각이 뒤로 집들이며 자신도 이따금씩 저를 박살일어주고 허버심심스럽게 대단히 불유쾌하고 상상도 없었다.\n",
            "그래서 내 두 눈도 당시 나는 가능한 한 가정집까지 품어 대며  수표를  보게로  만들고  있었다.\n",
            "웨믹이  내게  소시지  있던 드레스스 (카밀라, 주인공의 라이벌) 씨는 가슴 아래쪽에 있는 벽에 신이 요리된 곳이었다. 그런 비슷한 내용자신을 보면서 “자신의 걸음걸이 행운 보다 자신들의 목을 끌어당겼다. 그 후 그녀가 눈살을 찌푸리며 나를 어떻게 보이게도 가지고 있었다.\n",
            "‘조 부인과 조’(햄릿의 친구... 허버트의 사람 아버지의 묘지. / 드셔 보시는 23세. 서곡 〉 의 비밀이 아니라 수습공계약서를 주장 한 조각마다 이야기할 수 없습니다.)\n",
            "내가 하는 어떤 예리인 그녀  이름을 이렇게 덧붙였다.\n",
            "“제 대답은 당신들이 제지 그들을 무리한 사람이라면 약간 직무(약간)사조각을 받지 않은 것이오.  한 사람  ‘외설적인 수퇘지’ 여기에 살인 병으로  싸우주하게 여길 상황도 시작하지 않게나요. 제발 그 일에 있지 않았니?”\n",
            "“예, 마님.”\n",
            "“에스텔라, 어제 친척사로 하더라도 내가 너를 손짓하려고 긴답니다.”\n",
            "나는 조(매형)에게 그런 부름을  돌려  보았다.\n",
            "대장장이 출발 하나를 박수고 저녁식사였을 것이라고 생각하게 준다거나 혹은 ‘조 부인’(20살 많은 요소)의 지갑을 서로 \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1335506439208984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(np.random.randint(1, 1000))\n",
        "gen = GenerativeModel(model, vocab, temperature=0.8)\n",
        "import time\n",
        "start = time.time()\n",
        "print(gen.generate_n_chars(1000, \"사랑\"), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwGMIIchoy3D",
        "outputId": "32856661-ad7f-4bb4-b07a-ab9b6e519b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사랑과 친밀하게 말하는 것도  우릴  적어도  당신이 들었다.\n",
            "그가 이 모든 것을 지켜보았다. 그의 다음 그 어떤 것도  더 어울린지  표면서도 평상적으로 실행하기 시작했다.\n",
            "아버지에게 그 사실(이제) 그가 몰래  간을 수 있을 때까지 미스 해비샴도 재거스 씨의 의견을 제시했음에도 그녀는 에스텔라의 양복을 입과 위협에서 오래된 모습이 도움이 되었다는 인상을 받았기 때문임 / 이름 외국은 아니 연민에 따라 이 세상에서 가장 습지 한 것은 아닌지 나쁜 여성에 대해 충분히 불합되지 않는다면, 전 반드시의 여인도 모습 어슴이 조연이었다. 그래서 그는 지금 내 상속(부드러기)에서 애기를 띄고 사람들과 나만 음침하다는 것을 발견하지 못했다고 느꼈는지 그 아름다운 미인인지 이 강한 지위에 관여 그 어떤 부위는 연결 가 정보  간 혹은 이미 노의를 그려다보며 어떠한 말을 연설했던 것이라고 부를 수 없으리라는 것도 알 수 없음을 느꼈다.\n",
            "그래서 나는 그에게 수요하지 않을 수 없었다. 왜냐하면 에스텔라의 생각에 난 너도 그 이름을 벗어나지 않을 것이다.\n",
            "그녀의 그녀도 너만 나만큼이나 잘 알고 있어 그녀의 고개를 제 바닥 뒤쪽에 있는 태도로 이 적합하는 것은 태어나기 전에 에스텔라의 아름다움과 우리 모두 다 끝에서 그 남성이 동시에 그 여자애 같은 미소를 지어보이며 그에게 다시 도로하게 느끼고 있었다.\n",
            "내가 세상 속에서 가까이에 있는 멈칫하고 있었다. 그가 두 손을 호주머니에 찔러 넣었다. 에스텔라의 이름이 ‘분야 내리는 것’과 ‘다른 여운의 모습을’ 가까운 방에 나와 있  않을 뿐이야. 그걸 당 철이 있는 여느 때와 너에게 그 사람도 그 근처에도 너무 멀고 구혼이 되는  거야. 그래서 그가 오늘 밤에 손을 집어주고 무 시계를 뒤쫓고 있다고 생각하게도 오게 했지만.”\n",
            "“그래 넌 이미 네 다 아니에 너무 마신 파써서가 지나지, 이  오를 가게야 봐, 마을 사람들 마님도   세 개가 그  마술을  유용하고 있어, 비디, 조.”\n",
            "“그래 이 녀석, 그러니까 내가 너에게 너무도 잘 알고 있어 \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 7.547953367233276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(np.random.randint(1, 1000))\n",
        "gen = GenerativeModel(model, vocab, temperature=0.8)\n",
        "import time\n",
        "start = time.time()\n",
        "print(gen.generate_n_chars(1000, \"사랑\"), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t68gSwrT_7z",
        "outputId": "b896f01e-bb4a-46f1-cc4b-5a49d066555c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사랑하고 서명 재산을  받게 되는  대로  식탁하다’    에는  불평이  그  모로  수포나  있었다.\n",
            "“사실이지!” 누나가 발끈을 누르며 “아 따운 것 같아요.” 내가 말했다. “당신이 그를 두려워  나는  말씀드렸다.”\n",
            "“무슨 일이죠?” 누나가 물었다.\n",
            "“(주인공과 허버트)가 펜으마스에서 아주 어릴 적합니다. 하지만 내가 알기 쉽게 제지하는 의미야.”\n",
            "그때 그녀가 분명 내 휴식을 놓은 것 같은 태도로 나를 대답했다.\n",
            "“어서 일어나신다. 자, 어제 그녀를 찾는다면 그건 그렇게 될 겁니다.”\n",
            "“그 법적 조언자(변호사)가 일찍이 자실을 당하게 분명 재거스 씨께서 더 말씀드리요?”\n",
            "“재거스 씨, 그리고 경찰관들에 대한 지불 사는 에스텔라, 저 애가 우물쭈어  우리의  심장과  도움을 받을 수 있어.”  에게 부지러 가고 있었다. 누나는 그녀의 손을  만났다.\n",
            "하지만 아무리 그의 말에 따르면 “내 친구 그래서 이 마을에 내가 그를 가려다주었을 때, 미스 해비샴이 내게 미치고 있었음을 내가 다시 쳐다보며 말했다. “어쨌건 난 아냐! 그녀를 내가 널 당신하는 걸 보며 될 거야. 그래서 좀 더 어울 가진 남자들에게.”\n",
            "나는 이렇게 생각하지 않았다. 다만 그녀가 옆에서 일하고 있는 상태였던 그 모든 것을 지켜보았다.\n",
            "우리는 계속해서 나가게 되물었고, 그럼 프로비스(57세, 죄수)의 이위 그녀가 옆에 앉아 이로 바지 호주머니를 손으로 꼭 손으며 움츠러들었다.\n",
            "우리는 보트 안에서  “우리 집(고향 앞)에  다리를 풀고  놀라 어떻게  변했다.\n",
            "허영심 많은 사람이 살임을 차리는 아이들에게 완전하게 수행되면  될 수 있었던 것이다.\n",
            "우리는 마을로 자신의 커다란 모습을 보고 있는가 진입해 주중에 정말로 받았다.\n",
            "포함해서 물건들이 그려 자신의  마지막 남자에  있었다. 여름이 빙자 아래로 되었고 방금 여기저기에 둘러싸여 있었다.\n",
            "여기에 점점 더 밝게 빛의 보자를 가진 사람이었다. 하지만 약간 어디에 서 있었던 것은 저 사람이었다.\n",
            "앞서는 말이다. 그는 소리와 사람들 또래도 달래 가고  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1354598999023438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the generated text above, you can see that the model generates text that makes sense capturing dependencies between words and without any input. A simple n-gram model would have not been able to capture all of that in one sentence."
      ],
      "metadata": {
        "id": "eXDvsZsMuQv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a statistical method like the one you implemented in course 2 will not give you results that are as good as you saw here. Your model will not be able to encode information seen previously in the data set and as a result, the perplexity will increase. Remember from course 2 that the higher the perplexity, the worse your model is. Furthermore, statistical ngram models take up too much space and memory. As a result, they will be inefficient and too slow. Conversely, with deepnets, you can get a better perplexity. Note, learning about n-gram language models is still important and allows you to better understand deepnets."
      ],
      "metadata": {
        "id": "2xMnDC8zuRUb"
      }
    }
  ]
}